{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dad9adc9",
   "metadata": {},
   "source": [
    "# Wikipedia Web Scraping using Selenium + BeautifulSoup\n",
    "\n",
    "This notebook demonstrates a hybrid approach:\n",
    "- Selenium for page loading\n",
    "- BeautifulSoup for parsing\n",
    "- Pandas for data storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c305970",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Install required libraries (run once)\n",
    "# !pip install selenium beautifulsoup4 pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0738a42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa77c829",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Open Wikipedia page using Selenium\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://en.wikipedia.org/wiki/List_of_largest_companies_by_revenue\")\n",
    "driver.maximize_window()\n",
    "\n",
    "WebDriverWait(driver, 10).until(\n",
    "    EC.presence_of_element_located((By.CLASS_NAME, \"wikitable\"))\n",
    ")\n",
    "\n",
    "time.sleep(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2a4298",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pass Selenium page source to BeautifulSoup\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9321f33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Scrape table data\n",
    "table = soup.find(\"table\", class_=\"wikitable\")\n",
    "rows = table.find_all(\"tr\")\n",
    "\n",
    "companies = []\n",
    "\n",
    "for row in rows[1:]:\n",
    "    cols = row.find_all(\"td\")\n",
    "    if len(cols) >= 4:\n",
    "        companies.append({\n",
    "            \"Rank\": cols[0].text.strip(),\n",
    "            \"Company\": cols[1].text.strip(),\n",
    "            \"Revenue (USD million)\": cols[2].text.strip(),\n",
    "            \"Country\": cols[3].text.strip()\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c937a92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Close browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1031091e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(companies)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b051ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"wikipedia_companies.csv\", index=False)\n",
    "print(\"CSV file saved successfully\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}